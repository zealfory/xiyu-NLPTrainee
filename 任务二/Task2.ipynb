{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务二：基于深度学习的文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import data,datasets\n",
    "from torchtext.vocab import GloVe,FastText,CharNGram\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#from torch.autograd import Variable\n",
    "import torch\n",
    "#import sys\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU.\n"
     ]
    }
   ],
   "source": [
    "is_cuda = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda=True\n",
    "    print(\"Run on GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 数据集分割 train_set.csv and test_set.csv\n",
    "## 注意：只需运行一次\n",
    "\n",
    "# df_train = pd.read_csv(\"data_task1/train.tsv\", sep=\"\\t\")\n",
    "# X, y = df_train['Phrase'],df_train['Sentiment']\n",
    "# #print(X[:5],'\\n')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)\n",
    "# #print(X_train[:5], '\\n')\n",
    "# train = pd.concat([X_train,y_train], axis = 1)\n",
    "# #print(train[:5], '\\n')\n",
    "# train.to_csv('data_task1/train_set.csv')\n",
    "# test = pd.concat([X_test, y_test], axis = 1)\n",
    "# test.to_csv('data_task1/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, batch_first=True, fix_length=100)\n",
    "LABEL = data.Field(use_vocab = False, sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.TabularDataset(\n",
    "        path='data_task1/train_set.csv',format='csv',\n",
    "        skip_header=True,\n",
    "        fields = [('Index', None), ('Phrase', TEXT),('Sentiment',LABEL)])\n",
    "valid_data = data.TabularDataset(\n",
    "        path='data_task1/test_set.csv',format='csv',\n",
    "        skip_header=True,\n",
    "        fields = [('Index', None), ('Phrase', TEXT),('Sentiment',LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Phrase': ['there', \"'s\", 'no', 'energy', '.'], 'Sentiment': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, vectors = GloVe(name='6B', dim=300), min_freq = 10)\n",
    "LABEL.build_vocab(train_data, )\n",
    "\n",
    "BATCH_SIZE = 54\n",
    "\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    shuffle = True,\n",
    "    sort_key = lambda x: len(x.Phrase),\n",
    "    repeat = False)\n",
    "# train_iterator.repeat = False\n",
    "# valid_iterator.repeat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54, 100])\n"
     ]
    }
   ],
   "source": [
    "## Test\n",
    "batch = next(iter(train_iterator))\n",
    "batch.Phrase\n",
    "print(batch.Phrase.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 1, 4, 1, 2, 2, 3, 3, 2, 2, 2, 2, 1, 2, 2, 3, 2, 1, 2, 3, 1, 1,\n",
       "        2, 3, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 3, 4, 2, 3, 2, 2, 3, 2, 2,\n",
       "        3, 2, 3, 2, 3, 2], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8371\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(TEXT.vocab)\n",
    "print(n_vocab)\n",
    "n_hidden = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMRRnn(nn.Module):\n",
    "    '''  LSTM '''\n",
    "    def __init__(self, n_vocab, n_hidden, n_cat, bs = 1):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.bs = bs # batch_size\n",
    "        self.e = nn.Embedding(n_vocab, n_hidden) # embedding,[vocab_size, embed_dim]\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden) # [input_size, output_size]\n",
    "        self.fc2 = nn.Linear(n_hidden, n_cat) # [output_size, n_category] 5分类\n",
    "        self.softmax = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size()[0]\n",
    "        if bs != self.bs:\n",
    "            self.bs = bs\n",
    "        e_out = self.e(x) # [batch_size,seqlen,dim]\n",
    "        rnn_o, _ = self.rnn(e_out) # [batch_size,seqlen,output_dim]\n",
    "        mean_token_output = torch.mean(rnn_o, dim=1) # Mean\n",
    "        fc = F.dropout(self.fc2(mean_token_output), p = 0.1) # Dropout\n",
    "        return self.softmax(fc) # softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMRRnn(\n",
      "  (e): Embedding(8371, 1000)\n",
      "  (rnn): LSTM(1000, 1000)\n",
      "  (fc2): Linear(in_features=1000, out_features=5, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SAMRRnn(n_vocab, n_hidden, 5, 54) ## LSTM\n",
    "print(net)\n",
    "if is_cuda:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 使用预训练Embedding\n",
    "# net.e.weight.data = TEXT.vocab.vectors.cuda()\n",
    "# net.e.weight.requires_grad = False\n",
    "# optimizer = optim.Adam([param for param in net.parameters() if param.requires_grad == True],lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 1 is 1.13603887535111 and accuracy is 59221/109242 = 54%\n",
      "Training loss for epoch 2 is 0.9715498869099713 and accuracy is 66505/109242 = 60%\n",
      "Training loss for epoch 3 is 0.916336085795003 and accuracy is 69105/109242 = 63%\n",
      "Training loss for epoch 4 is 0.8851783196030995 and accuracy is 70590/109242 = 64%\n",
      "Training loss for epoch 5 is 0.8619078225531286 and accuracy is 71719/109242 = 65%\n"
     ]
    }
   ],
   "source": [
    "## For training\n",
    "for epoch in range(5):\n",
    "    \n",
    "    net.train() # 训练模式\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        text, label = batch.Phrase , batch.Sentiment\n",
    "        if torch.cuda.is_available():\n",
    "            text, label = text.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        prediction = net(text)\n",
    "        \n",
    "        loss = F.nll_loss(prediction,label)\n",
    "        \n",
    "        running_loss += F.nll_loss(prediction,label,size_average=False).item()\n",
    "        preds = prediction.data.max(dim=1,keepdim=True)[1]\n",
    "        running_correct += preds.eq(label.data.view_as(preds)).cpu().sum()\n",
    "            \n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss = running_loss / len(train_iterator.dataset)\n",
    "    accuracy = 100.0 * running_correct / len(train_iterator.dataset)\n",
    "    \n",
    "    print(f'Training loss for epoch {epoch + 1} is {loss} and accuracy is {running_correct}/{len(train_iterator.dataset)} = {accuracy}%')\n",
    "        \n",
    "#     prediction = net(batch.Phrase)\n",
    "    \n",
    "#     loss = F.nll_loss(prediction,batch.Sentiment)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if t % 5 == 0:\n",
    "#         print('Loss:', loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss is 1.0688864415721313 and accuracy is 28547/46818 = 60%\n"
     ]
    }
   ],
   "source": [
    "## For validating\n",
    "    \n",
    "net.eval() #测试模式\n",
    "volatile = True\n",
    "    \n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "    \n",
    "for batch_idx, batch in enumerate(valid_iterator):\n",
    "    text, label = batch.Phrase , batch.Sentiment\n",
    "    if torch.cuda.is_available():\n",
    "        text, label = text.cuda(), label.cuda()\n",
    "            \n",
    "#       optimizer.zero_grad()\n",
    "    prediction = net(text)\n",
    "        \n",
    "    loss = F.nll_loss(prediction,label)\n",
    "        \n",
    "    running_loss += F.nll_loss(prediction,label,size_average=False).item()\n",
    "    preds = prediction.data.max(dim=1,keepdim=True)[1]\n",
    "    running_correct += preds.eq(label.data.view_as(preds)).cpu().sum()\n",
    "            \n",
    "            \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "loss = running_loss/len(valid_iterator.dataset)\n",
    "accuracy = 100. * running_correct/len(valid_iterator.dataset)\n",
    "    \n",
    "print(f'Valid loss is {loss} and accuracy is {running_correct}/{len(valid_iterator.dataset)} = {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMRCnn(nn.Module):\n",
    "    '''CNN'''\n",
    "    def __init__(self, n_vocab, n_hidden, n_cat, n_kernel, kernel_sizes, bs = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        Vocab = n_vocab ## 已知词的数量\n",
    "        Dim = n_hidden ##每个词向量长度\n",
    "        Cla = n_cat  ##类别数\n",
    "        Ci = 1  ##输入的channel数\n",
    "        Knum = n_kernel ## 每种卷积核的数量\n",
    "        Ks = kernel_sizes ## 卷积核list，形如[2,3,4]\n",
    "        self.bs = bs\n",
    "        self.embed = nn.Embedding(Vocab,Dim) ## 词向量，这里直接随机\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv2d(Ci,Knum,(K,Dim)) for K in Ks]) ## 卷积层\n",
    "        self.dropout = nn.Dropout(args.dropout) \n",
    "        self.fc = nn.Linear(len(Ks)*Knum,Cla) ##全连接层\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embed(x) #(N,W,D)\n",
    "        \n",
    "        x = x.unsqueeze(1) #(N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] # len(Ks)*(N,Knum,W)\n",
    "        x = [F.max_pool1d(line,line.size(2)).squeeze(2) for line in x]  # len(Ks)*(N,Knum)\n",
    "        \n",
    "        x = torch.cat(x,1) #(N,Knum*len(Ks))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        logit = self.fc(x)\n",
    "        return logit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
