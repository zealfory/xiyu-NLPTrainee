{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务一：基于机器学习的文本分类\n",
    "### 2019-07-15 ~ 2019-07-18\n",
    "实现基于logistic/softmax regression的文本分类<br>\n",
    "\n",
    "#### 参考\n",
    "\n",
    "文本分类<br>\n",
    "《神经网络与深度学习》 第2/3章<br>\n",
    "#### 数据集：Classify the sentiment of sentences from the Rotten Tomatoes dataset\n",
    "\n",
    "#### 实现要求：NumPy\n",
    "\n",
    "#### 需要了解的知识点：\n",
    "\n",
    "文本特征表示：Bag-of-Word，N-gram<br>\n",
    "分类器：logistic/softmax regression，损失函数、（随机）梯度下降、特征选择<br>\n",
    "数据集：训练集/验证集/测试集的划分<br>\n",
    "#### 实验：\n",
    "\n",
    "分析不同的特征、损失函数、学习率对最终分类性能的影响<br>\n",
    "shuffle 、batch、mini-batch<br>\n",
    "#### 时间：两周"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 文本特征表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re, string\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "df_train = pd.read_csv(\"data_task1/train.tsv\", sep=\"\\t\")\n",
    "df_test = pd.read_csv(\"data_task1/test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据清洗\n",
    "def to_lowercase(words):\n",
    "    \"\"\" 转换为小写 \"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\" 去除标点符号 \"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\" 去除数字 \"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(\"\\d+\", \"\", word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\" 去除停用词 \"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\" 词形还原 \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def preprocess(words):\n",
    "    \"\"\" 封装以上操作 \"\"\"\n",
    "    #words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    #words = lemmatize_verbs(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [series, escapades, demonstrating, adage, good...\n",
       "1    [series, escapades, demonstrating, adage, good...\n",
       "2                                             [series]\n",
       "3                                                   []\n",
       "4                                             [series]\n",
       "Name: Words, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "# 1.分词\n",
    "df_train['Words'] = df_train['Phrase'].apply(nltk.word_tokenize)\n",
    "\n",
    "\n",
    "# 2.预处理\n",
    "df_train['Words'] = df_train['Words'].apply(preprocess) \n",
    "df_train['Words'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[series, escapades, demonstrating, adage, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[series, escapades, demonstrating, adage, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment                                              Words  \n",
       "0          1  [series, escapades, demonstrating, adage, good...  \n",
       "1          2  [series, escapades, demonstrating, adage, good...  \n",
       "2          2                                           [series]  \n",
       "3          2                                                 []  \n",
       "4          2                                           [series]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'watching', 'without', 'human', 'never', 'nothing', 'times', 'scenes', 'years', 'young', 'almost', 'moments', 'watch', 'dialogue', 'rrb', 'also', 'comes', 'good', 'kind', 'old', 'tale', 'could', 'story', 'movie', 'hard', 'quite', 'take', 'may', 'best', 'right', 'audience', 'script', 'rather', 'lot', 'cinema', 'yet', 'world', 'emotional', 'seems', 'performances', 'work', 'nt', 'humor', 'like', 'romantic', 'bad', 'screen', 'sense', 'subject', 'fun', 'interesting', 'even', 'thriller', 'movies', 'little', 'end', 'something', 'ever', 'heart', 'still', 'plot', 'new', 'documentary', 'look', 'well', 'far', 'makes', 'love', 'man', 'people', 'way', 'enough', 'entertaining', 'back', 'great', 'comedy', 'full', 'two', 'american', 'might', 'much', 'life', 'picture', 'get', 'acting', 'thing', 'material', 'another', 'lrb', 'less', 'hollywood', 'funny', 'action', 'time', 'long', 'first', 'find', 'see', 'often', 'make', 'film', 'performance', 'actors', 'really', 'year', 'going', 'come', 'would', 'better', 'cast', 'one', 'music', 'big', 'drama', 'go', 'seen', 'films', 'worth', 'minutes', 'things', 'original', 'character', 'every', 'real', 'characters', 'feel', 'many', 'director', 'family', 'made', 'us'}\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# # 3.构建词表\n",
    "# word_set = set()\n",
    "# for l in df_train['Words']:\n",
    "#     for e in l:\n",
    "#         if e != \"\":\n",
    "#             word_set.add(e)\n",
    "# 3.构建词典\n",
    "word_set = set()\n",
    "word_dict = {}\n",
    "for l in df_train['Words']:\n",
    "    for e in l:\n",
    "        if e != \"\":\n",
    "            if e not in word_dict:\n",
    "                word_dict[e] = 1\n",
    "            else:\n",
    "                word_dict[e] += 1\n",
    "\n",
    "# 词表维数过高，只选择频数>1000的词作为特征                \n",
    "for k, v in word_dict.items():\n",
    "    if v > 500:\n",
    "        word_set.add(k)\n",
    "    \n",
    "print(word_set)\n",
    "        \n",
    "# 4.将词项映射到ID        \n",
    "word_to_int = {word: i for i, word in enumerate(word_set, 1)}\n",
    "def get_tokens(l):\n",
    "    l_new = []\n",
    "    for word in l:\n",
    "        if word in word_to_int:\n",
    "            l_new.append(word_to_int[word])\n",
    "        else:\n",
    "            l_new.append('unk')\n",
    "    return l_new\n",
    "#df_train['Tokens'] = df_train['Words'].apply(lambda l: [word_to_int[word] for word in l])\n",
    "df_train['Tokens'] = df_train['Words'].apply(get_tokens)\n",
    "#print(df_train['Tokens'].head())\n",
    "#print(df_train.head())\n",
    "\n",
    "                                                \n",
    "# 5.求最长Phrases所包含的词项，其他末尾添0\n",
    "max_len = df_train['Tokens'].str.len().max()\n",
    "print(max_len)\n",
    "\n",
    "# all_tokens = np.array([t for t in df_train['Tokens']])\n",
    "# encoded_labels = np.array([l for l in df_train['Sentiment']])\n",
    "\n",
    "# features = np.zeros((len(all_tokens), max_len), dtype=int)\n",
    "# for i, row in enumerate(all_tokens):\n",
    "#     features[i, :len(row)] = row\n",
    "\n",
    "\n",
    "# print(features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复采样\n",
    "sent_2 = df_train[df_train['Sentiment']==2]\n",
    "#we will copy class 0 11 times\n",
    "sent_0 = df_train[df_train['Sentiment']==0]\n",
    "#we will copy class 1 2 times\n",
    "sent_1 = df_train[df_train['Sentiment']==1]\n",
    "#we will copy class 3 2 times\n",
    "sent_3 = df_train[df_train['Sentiment']==3]\n",
    "#we will copy class 4 8 times\n",
    "sent_4 = df_train[df_train['Sentiment']==4]\n",
    "\n",
    "#-----------------------------------------------------\n",
    "df_train1 = df_train.append([sent_0,sent_0,sent_0,sent_0,sent_0,sent_0,sent_0,sent_0,sent_0,sent_0])\n",
    "df_train1 = df_train.append([sent_1])\n",
    "df_train1 = df_train.append([sent_3])\n",
    "df_train1 = df_train.append([sent_4,sent_4,sent_4,sent_4,sent_4,sent_4,sent_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['unk', 'unk', 'unk', 'unk', 17, 'unk', 15, 17, 'unk', 'unk', 'unk', 'unk', 'unk', 80, 22])\n",
      " list(['unk', 'unk', 'unk', 'unk', 17, 'unk']) list(['unk']) ...\n",
      " list(['unk', 'unk'])\n",
      " list([103, 74, 'unk', 'unk', 124, 37, 'unk', 'unk', 'unk'])\n",
      " list([74, 'unk', 'unk', 124, 37, 'unk', 'unk', 'unk'])]\n"
     ]
    }
   ],
   "source": [
    "all_tokens = np.array([t for t in df_train1['Tokens']])\n",
    "encoded_labels = np.array([l for l in df_train1['Sentiment']])\n",
    "\n",
    "print(all_tokens)\n",
    "embed = np.zeros((len(all_tokens), len(word_set)), dtype = int)\n",
    "for i, row in enumerate(all_tokens):\n",
    "    for x in row:\n",
    "        if x != 'unk':\n",
    "            embed[i, x-1] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练 & 验证集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(154351, 130) \n",
      "Test set: \t\t(66151, 130)\n"
     ]
    }
   ],
   "source": [
    "# split_frac = 0.8\n",
    "\n",
    "# split_idx = int(len(features)*0.8)\n",
    "\n",
    "# train_x, test_x = embed[:split_idx], embed[split_idx:]\n",
    "# train_y, test_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "train_x, test_x, train_y, test_y = train_test_split(embed, encoded_labels, test_size=0.3, random_state = 0, shuffle = True)\n",
    " \n",
    "\n",
    "## print out the shapes of  resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 查看维度 & 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 154351\n",
      "Number of testing examples: m_test = 66151\n",
      "Width of each example: num_word = 130\n",
      "train_x shape: (154351, 130)\n",
      "train_y shape: (154351,)\n",
      "test_x shape: (66151, 130)\n",
      "test_y shape: (66151,)\n"
     ]
    }
   ],
   "source": [
    "m_train = len(train_x)\n",
    "m_test = len(test_x)\n",
    "num_word = train_x.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Width of each example: num_word = \" + str(num_word))\n",
    "print (\"train_x shape: \" + str(train_x.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x shape: \" + str(test_x.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把labels转成one-hot格式\n",
    "def convert_to_onehot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_flatten shape:  (130, 154351)\n",
      "train_y_onehot shape:  (5, 154351)\n",
      "test_x_flatten shape:  (130, 66151)\n",
      "test_y_onehot shape:  (5, 66151)\n"
     ]
    }
   ],
   "source": [
    "train_x_transpose = train_x.T\n",
    "test_x_transpose = test_x.T\n",
    "\n",
    "train_y_onehot = convert_to_onehot(train_y, 5)\n",
    "test_y_onehot = convert_to_onehot(test_y, 5)\n",
    "\n",
    "print (\"train_x_flatten shape: \",train_x_transpose.shape)\n",
    "print (\"train_y_onehot shape: \" ,train_y_onehot.shape)\n",
    "print (\"test_x_flatten shape: \" ,test_x_transpose.shape)\n",
    "print (\"test_y_onehot shape: \",test_y_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 分类器\n",
    "\n",
    "模型架构及参数选择：\n",
    "1. 模型框架：input layer — hidden layer (128 units, relu） — output layer(5 units, softmax)\n",
    "2. 激活函数：relu和softmax\n",
    "3. 参数初始化方式：随机初始化\n",
    "4. 损失函数：softmax交叉熵\n",
    "5. 优化算法：梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 激活函数 ReLU & softMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    ReLU函数\n",
    "    Z -- 线性输出\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp(z)容易造成数值上溢\n",
    "# 可以证明softmax(z) = soft(z+c),其中c为常数，即softmax对input加减一个常数项具有不变性。\n",
    "# 因此，z = z - max(z)\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    softmax函数\n",
    "    \"\"\"\n",
    "    Z_shift = Z - np.max(Z, axis = 0)\n",
    "    A = np.exp(Z_shift) / np.sum(np.exp(Z_shift), axis=0)\n",
    "    \n",
    "    cache = Z_shift\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid函数\n",
    "    \"\"\"\n",
    "    A = 1.0 / (1 + np.exp(-Z))\n",
    "    \n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数随机初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    n_x -- 输入层大小\n",
    "    n_h -- 隐藏层大小\n",
    "    n_y -- 输出层大小\n",
    "    \n",
    "    返回：\n",
    "    W1 -- 权重矩阵，(n_h, n_x)\n",
    "    b1 -- 偏置向量，(n_h, 1)\n",
    "    W2 -- 权重矩阵 (n_y, n_h)\n",
    "    b2 -- 偏置向量 (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01   \n",
    "    b1 = np.zeros((n_h, 1))                 \n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前向传播\n",
    "\n",
    "\n",
    "线性部分：$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
    "\n",
    "\n",
    "where $A^{[0]} = X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    前向传播--线性部分,加权求和\n",
    "\n",
    "    Arguments:\n",
    "    A -- 当前输入\n",
    "    W -- 权重矩阵\n",
    "    b -- 偏置\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非线性部分：  $$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$$ <br>\n",
    "   \"g\" -- softmax() or relu()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    前向传播--非线性部分，经过激活函数\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"softmax\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "      \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- 预测概率\n",
    "    Y -- 真实标记\n",
    "\n",
    "    Returns:\n",
    "    cost -- 交叉熵损失\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = -(np.sum(Y * np.log(AL))) / float(m)\n",
    "    #cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    线性偏导\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / float(m)\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / float(m)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    relu偏导\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, cache):\n",
    "    \"\"\"\n",
    "    链式法则dL/dZ = （dL/dA） * （dA/dZ）\n",
    "    \"\"\"\n",
    "    Z = cache  #Z_shift\n",
    "    \n",
    "    s = np.exp(Z)/ np.sum(np.exp(Z), axis=0)\n",
    "    dZ = s - Y\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    激活函数偏导\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    \"\"\"\n",
    "    利用梯度更新参数\n",
    "    \"\"\" \n",
    "    L = len(parameters) // 2 # 每层参数个数\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        parameters['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总体模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.1, num_iterations = 1000, print_cost=False):\n",
    "    \"\"\"\n",
    "    两层神经网络\n",
    "    返回权重和偏置\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              \n",
    "    m = X.shape[1]                          \n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # 参数初始化\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # 迭代\n",
    "    for i in range(0, num_iterations):\n",
    "        # mini-batch\n",
    "#         start = (i * batch_size) % m\n",
    "#         end = min(start + batch_size , m)\n",
    "\n",
    "#         X_p = X[:, start: end]\n",
    "#         Y_p = Y[:, start: end]\n",
    "        # 前向传播\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation='relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation='softmax')\n",
    "\n",
    "        # 计算损失\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # 误差反向传播\n",
    "        dA1, dW2, db2 = linear_activation_backward(Y, cache2, activation='softmax')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation='relu')\n",
    "        \n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # 更新参数\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # 输出损失\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # 画图\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations(* 100)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 154351)\n"
     ]
    }
   ],
   "source": [
    "print(train_x_transpose.shape)\n",
    "train_data_x = train_x_transpose\n",
    "train_data_y = train_y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 训练模型获取参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.6094781967375478\n",
      "Cost after iteration 100: 1.4022126005080553\n",
      "Cost after iteration 200: 1.3896550868164732\n",
      "Cost after iteration 300: 1.386052053257626\n",
      "Cost after iteration 400: 1.3838850140183125\n",
      "Cost after iteration 500: 1.3814843803972054\n",
      "Cost after iteration 600: 1.3779390462871317\n",
      "Cost after iteration 700: 1.3725400904650225\n",
      "Cost after iteration 800: 1.3650067393551821\n",
      "Cost after iteration 900: 1.3559298427518331\n",
      "Cost after iteration 1000: 1.3465672595826004\n",
      "Cost after iteration 1100: 1.3379609383216826\n",
      "Cost after iteration 1200: 1.3304378235842458\n",
      "Cost after iteration 1300: 1.3239498903952511\n",
      "Cost after iteration 1400: 1.3184555645575742\n",
      "Cost after iteration 1500: 1.3138959624457036\n",
      "Cost after iteration 1600: 1.3101919545569056\n",
      "Cost after iteration 1700: 1.3072201777624672\n",
      "Cost after iteration 1800: 1.3048537755324616\n",
      "Cost after iteration 1900: 1.3029566468438458\n",
      "Cost after iteration 2000: 1.3014147330447516\n",
      "Cost after iteration 2100: 1.300132584793912\n",
      "Cost after iteration 2200: 1.2990389115298127\n",
      "Cost after iteration 2300: 1.2980772221079728\n",
      "Cost after iteration 2400: 1.2972100706122127\n",
      "Cost after iteration 2500: 1.2964056356931732\n",
      "Cost after iteration 2600: 1.2956396488922108\n",
      "Cost after iteration 2700: 1.2948995221079262\n",
      "Cost after iteration 2800: 1.2941710748973538\n",
      "Cost after iteration 2900: 1.293444794325502\n",
      "Cost after iteration 3000: 1.2927157751051108\n",
      "Cost after iteration 3100: 1.2919794886414553\n",
      "Cost after iteration 3200: 1.291232523477644\n",
      "Cost after iteration 3300: 1.2904752949147529\n",
      "Cost after iteration 3400: 1.2897103761172273\n",
      "Cost after iteration 3500: 1.2889406638988328\n",
      "Cost after iteration 3600: 1.2881685009168218\n",
      "Cost after iteration 3700: 1.2873981607546359\n",
      "Cost after iteration 3800: 1.2866326211877497\n",
      "Cost after iteration 3900: 1.2858794838433036\n",
      "Cost after iteration 4000: 1.2851438860931124\n",
      "Cost after iteration 4100: 1.284429050321932\n",
      "Cost after iteration 4200: 1.2837374089532279\n",
      "Cost after iteration 4300: 1.2830716343819197\n",
      "Cost after iteration 4400: 1.2824330694685508\n",
      "Cost after iteration 4500: 1.2818226779390878\n",
      "Cost after iteration 4600: 1.2812389116794358\n",
      "Cost after iteration 4700: 1.2806806607359071\n",
      "Cost after iteration 4800: 1.2801459701384903\n",
      "Cost after iteration 4900: 1.2796334184662992\n",
      "Cost after iteration 5000: 1.279141927050959\n",
      "Cost after iteration 5100: 1.2786695740612688\n",
      "Cost after iteration 5200: 1.278214779586761\n",
      "Cost after iteration 5300: 1.2777765775537466\n",
      "Cost after iteration 5400: 1.2773540785871078\n",
      "Cost after iteration 5500: 1.276945773782538\n",
      "Cost after iteration 5600: 1.2765508654842634\n",
      "Cost after iteration 5700: 1.2761682623471287\n",
      "Cost after iteration 5800: 1.2757971407321034\n",
      "Cost after iteration 5900: 1.2754369865540538\n",
      "Cost after iteration 6000: 1.2750874222182766\n",
      "Cost after iteration 6100: 1.2747479189059185\n",
      "Cost after iteration 6200: 1.274417806782159\n",
      "Cost after iteration 6300: 1.2740961320193651\n",
      "Cost after iteration 6400: 1.2737828420509394\n",
      "Cost after iteration 6500: 1.2734769559533203\n",
      "Cost after iteration 6600: 1.273177861618514\n",
      "Cost after iteration 6700: 1.27288551187164\n",
      "Cost after iteration 6800: 1.2725993988547133\n",
      "Cost after iteration 6900: 1.2723190848708292\n",
      "Cost after iteration 7000: 1.2720449165569627\n",
      "Cost after iteration 7100: 1.2717769274424238\n",
      "Cost after iteration 7200: 1.2715146429781397\n",
      "Cost after iteration 7300: 1.2712580438520422\n",
      "Cost after iteration 7400: 1.2710069674388587\n",
      "Cost after iteration 7500: 1.2707609219104827\n",
      "Cost after iteration 7600: 1.2705198040041183\n",
      "Cost after iteration 7700: 1.2702835070301606\n",
      "Cost after iteration 7800: 1.2700514309113142\n",
      "Cost after iteration 7900: 1.269823736891613\n",
      "Cost after iteration 8000: 1.2696005098283913\n",
      "Cost after iteration 8100: 1.269381041384441\n",
      "Cost after iteration 8200: 1.26916538221458\n",
      "Cost after iteration 8300: 1.2689531692402358\n",
      "Cost after iteration 8400: 1.2687448439270002\n",
      "Cost after iteration 8500: 1.268540418812653\n",
      "Cost after iteration 8600: 1.2683393789808062\n",
      "Cost after iteration 8700: 1.268141546533017\n",
      "Cost after iteration 8800: 1.267947422828982\n",
      "Cost after iteration 8900: 1.2677567641364813\n",
      "Cost after iteration 9000: 1.2675695467029406\n",
      "Cost after iteration 9100: 1.2673847227969084\n",
      "Cost after iteration 9200: 1.267202987563367\n",
      "Cost after iteration 9300: 1.267024350308046\n",
      "Cost after iteration 9400: 1.2668481926929926\n",
      "Cost after iteration 9500: 1.2666744959116516\n",
      "Cost after iteration 9600: 1.2665032482001684\n",
      "Cost after iteration 9700: 1.2663345559336983\n",
      "Cost after iteration 9800: 1.26616810148084\n",
      "Cost after iteration 9900: 1.266004051471141\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model(train_data_x, train_data_y, layers_dims = (130, 128, 5), learning_rate = 0.1,\n",
    "                             num_iterations = 10000, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(X, y, parameters):\n",
    "    \"\"\"\n",
    "    预测类别，计算精度\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Forward propagation\n",
    "    A1, _ = linear_activation_forward(X, W1, b1, activation='relu')\n",
    "    probs, _ = linear_activation_forward(A1, W2, b2, activation='softmax')\n",
    "    \n",
    "    # convert probas to 0-4 predictions\n",
    "    predict_label = np.argmax(probs, axis=0)\n",
    "#     for i in predict_label:\n",
    "#         if i != 2:\n",
    "#             print(i)\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((predict_label == y)/float(m))))\n",
    "        \n",
    "    return predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4976493174706354\n"
     ]
    }
   ],
   "source": [
    "# parameters = initialize_parameters(50, 128, 5)\n",
    "prediction = predict_labels(test_x_transpose, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 真实测试数据 & 结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_without_y(X, parameters):\n",
    "    \"\"\"\n",
    "    对无标记数据预测类别\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Forward propagation\n",
    "    A1, _ = linear_activation_forward(X, W1, b1, activation='relu')\n",
    "    probs, _ = linear_activation_forward(A1, W2, b2, activation='softmax')\n",
    "    \n",
    "    # convert probas to 0-4 predictions\n",
    "    predict_label = np.argmax(probs, axis=0)\n",
    "    for i in predict_label:\n",
    "        if i != 2:\n",
    "            print(i)\n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    #print(\"Accuracy: \"  + str(np.sum((predict_label == y)/float(m))))\n",
    "        \n",
    "    return predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试集\n",
    "from numpy import savetxt\n",
    "df_test['Words'] = df_test['Phrase'].apply(nltk.word_tokenize)\n",
    "\n",
    "df_test['Words'] = df_test['Words'].apply(normalize) \n",
    "df_test['Words'].head()\n",
    "\n",
    "df_test['Tokens'] = df_train['Words'].apply(get_tokens)\n",
    "\n",
    "\n",
    "test_tokens = np.array([t for t in df_test['Tokens']])\n",
    "\n",
    "test_embed = np.zeros((len(test_tokens), len(word_set)), dtype = int)\n",
    "\n",
    "for i, row in enumerate(test_tokens):\n",
    "    for x in row:\n",
    "        if x != 'unk':\n",
    "            embed[i, x-1] += 1\n",
    "\n",
    "df_test_x = test_embed.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predict_labels_without_y(df_test_x, parameters)\n",
    "pred = [[index + 156061,x] for index, x in enumerate(predicted)]\n",
    "savetxt('zzmsampleSubmission.csv',pred,delimiter=',',fmt='%d,%d',header='PhraseId,Sentiment',comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
